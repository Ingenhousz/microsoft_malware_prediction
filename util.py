import sys
sys.path.append("/sfs/qumulo/qhome/yx3fz/.local/lib/python3.6/site-packages")

import pickle
import logging
import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from sklearn import metrics
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier

from sklearn.ensemble import RandomForestClassifier
from datetime import datetime, date, timedelta

import random

from scipy.sparse import vstack, csr_matrix, save_npz, load_npz
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
import gc
gc.enable()

from contextlib import contextmanager
import time

logger = logging.getLogger(__name__)
logging.basicConfig(
    format='[%(levelname)s] %(asctime)s %(filename)s: %(lineno)d: %(message)s',
    datefmt='%Y-%m-%d:%H:%M:%S',
    level=logging.DEBUG
    )


@contextmanager
def timer(title):
    t0 = time.time()
    yield
    logger.info("{} - done in {:.0f}s".format(title, time.time() - t0))


def sample_train(s=10000):

    dtypes = {
        'MachineIdentifier': 'category',
        'ProductName': 'category',
        'EngineVersion': 'category',
        'AppVersion': 'category',
        'AvSigVersion': 'category',
        'IsBeta': 'int8',
        'RtpStateBitfield': 'float16',
        'IsSxsPassiveMode': 'int8',
        'DefaultBrowsersIdentifier': 'float16',
        'AVProductStatesIdentifier': 'float32',
        'AVProductsInstalled': 'float16',
        'AVProductsEnabled': 'float16',
        'HasTpm': 'int8',
        'CountryIdentifier': 'int16',
        'CityIdentifier': 'float32',
        'OrganizationIdentifier': 'float16',
        'GeoNameIdentifier': 'float16',
        'LocaleEnglishNameIdentifier': 'int8',
        'Platform': 'category',
        'Processor': 'category',
        'OsVer': 'category',
        'OsBuild': 'int16',
        'OsSuite': 'int16',
        'OsPlatformSubRelease': 'category',
        'OsBuildLab': 'category',
        'SkuEdition': 'category',
        'IsProtected': 'float16',
        'AutoSampleOptIn': 'int8',
        'PuaMode': 'category',
        'SMode': 'float16',
        'IeVerIdentifier': 'float16',
        'SmartScreen': 'category',
        'Firewall': 'float16',
        'UacLuaenable': 'float32',
        'Census_MDC2FormFactor': 'category',
        'Census_DeviceFamily': 'category',
        'Census_OEMNameIdentifier': 'float16',
        'Census_OEMModelIdentifier': 'float32',
        'Census_ProcessorCoreCount': 'float16',
        'Census_ProcessorManufacturerIdentifier': 'float16',
        'Census_ProcessorModelIdentifier': 'float16',
        'Census_ProcessorClass': 'category',
        'Census_PrimaryDiskTotalCapacity': 'float32',
        'Census_PrimaryDiskTypeName': 'category',
        'Census_SystemVolumeTotalCapacity': 'float32',
        'Census_HasOpticalDiskDrive': 'int8',
        'Census_TotalPhysicalRAM': 'float32',
        'Census_ChassisTypeName': 'category',
        'Census_InternalPrimaryDiagonalDisplaySizeInInches': 'float16',
        'Census_InternalPrimaryDisplayResolutionHorizontal': 'float16',
        'Census_InternalPrimaryDisplayResolutionVertical': 'float16',
        'Census_PowerPlatformRoleName': 'category',
        'Census_InternalBatteryType': 'category',
        'Census_InternalBatteryNumberOfCharges': 'float32',
        'Census_OSVersion': 'category',
        'Census_OSArchitecture': 'category',
        'Census_OSBranch': 'category',
        'Census_OSBuildNumber': 'int16',
        'Census_OSBuildRevision': 'int32',
        'Census_OSEdition': 'category',
        'Census_OSSkuName': 'category',
        'Census_OSInstallTypeName': 'category',
        'Census_OSInstallLanguageIdentifier': 'float16',
        'Census_OSUILocaleIdentifier': 'int16',
        'Census_OSWUAutoUpdateOptionsName': 'category',
        'Census_IsPortableOperatingSystem': 'int8',
        'Census_GenuineStateName': 'category',
        'Census_ActivationChannel': 'category',
        'Census_IsFlightingInternal': 'float16',
        'Census_IsFlightsDisabled': 'float16',
        'Census_FlightRing': 'category',
        'Census_ThresholdOptIn': 'float16',
        'Census_FirmwareManufacturerIdentifier': 'float16',
        'Census_FirmwareVersionIdentifier': 'float32',
        'Census_IsSecureBootEnabled': 'int8',
        'Census_IsWIMBootEnabled': 'float16',
        'Census_IsVirtualDevice': 'float16',
        'Census_IsTouchEnabled': 'int8',
        'Census_IsPenCapable': 'int8',
        'Census_IsAlwaysOnAlwaysConnectedCapable': 'float16',
        'Wdft_IsGamer': 'float16',
        'Wdft_RegionIdentifier': 'float16',
        'HasDetections': 'int8'
    }

    filename = 'input/train.csv'
    n = sum(1 for line in open(filename)) - 1  # number of records in file (excludes header)
    skip = sorted(random.sample(range(1, n + 1), n - s))  # the 0-indexed header will not be included in the skip list
    train = pd.read_csv(filename, skiprows=skip, low_memory=True)

    return train

def read_files(debug=True):
    num_row = 50000 if debug else None
    dtypes = {
        'MachineIdentifier': 'category',
        'ProductName': 'category',
        'EngineVersion': 'category',
        'AppVersion': 'category',
        'AvSigVersion': 'category',
        'IsBeta': 'int8',
        'RtpStateBitfield': 'float16',
        'IsSxsPassiveMode': 'int8',
        'DefaultBrowsersIdentifier': 'float16',
        'AVProductStatesIdentifier': 'float32',
        'AVProductsInstalled': 'float16',
        'AVProductsEnabled': 'float16',
        'HasTpm': 'int8',
        'CountryIdentifier': 'int16',
        'CityIdentifier': 'float32',
        'OrganizationIdentifier': 'float16',
        'GeoNameIdentifier': 'float16',
        'LocaleEnglishNameIdentifier': 'int8',
        'Platform': 'category',
        'Processor': 'category',
        'OsVer': 'category',
        'OsBuild': 'int16',
        'OsSuite': 'int16',
        'OsPlatformSubRelease': 'category',
        'OsBuildLab': 'category',
        'SkuEdition': 'category',
        'IsProtected': 'float16',
        'AutoSampleOptIn': 'int8',
        'PuaMode': 'category',
        'SMode': 'float16',
        'IeVerIdentifier': 'float16',
        'SmartScreen': 'category',
        'Firewall': 'float16',
        'UacLuaenable': 'float32',
        'Census_MDC2FormFactor': 'category',
        'Census_DeviceFamily': 'category',
        'Census_OEMNameIdentifier': 'float16',
        'Census_OEMModelIdentifier': 'float32',
        'Census_ProcessorCoreCount': 'float16',
        'Census_ProcessorManufacturerIdentifier': 'float16',
        'Census_ProcessorModelIdentifier': 'float16',
        'Census_ProcessorClass': 'category',
        'Census_PrimaryDiskTotalCapacity': 'float32',
        'Census_PrimaryDiskTypeName': 'category',
        'Census_SystemVolumeTotalCapacity': 'float32',
        'Census_HasOpticalDiskDrive': 'int8',
        'Census_TotalPhysicalRAM': 'float32',
        'Census_ChassisTypeName': 'category',
        'Census_InternalPrimaryDiagonalDisplaySizeInInches': 'float16',
        'Census_InternalPrimaryDisplayResolutionHorizontal': 'float16',
        'Census_InternalPrimaryDisplayResolutionVertical': 'float16',
        'Census_PowerPlatformRoleName': 'category',
        'Census_InternalBatteryType': 'category',
        'Census_InternalBatteryNumberOfCharges': 'float32',
        'Census_OSVersion': 'category',
        'Census_OSArchitecture': 'category',
        'Census_OSBranch': 'category',
        'Census_OSBuildNumber': 'int16',
        'Census_OSBuildRevision': 'int32',
        'Census_OSEdition': 'category',
        'Census_OSSkuName': 'category',
        'Census_OSInstallTypeName': 'category',
        'Census_OSInstallLanguageIdentifier': 'float16',
        'Census_OSUILocaleIdentifier': 'int16',
        'Census_OSWUAutoUpdateOptionsName': 'category',
        'Census_IsPortableOperatingSystem': 'int8',
        'Census_GenuineStateName': 'category',
        'Census_ActivationChannel': 'category',
        'Census_IsFlightingInternal': 'float16',
        'Census_IsFlightsDisabled': 'float16',
        'Census_FlightRing': 'category',
        'Census_ThresholdOptIn': 'float16',
        'Census_FirmwareManufacturerIdentifier': 'float16',
        'Census_FirmwareVersionIdentifier': 'float32',
        'Census_IsSecureBootEnabled': 'int8',
        'Census_IsWIMBootEnabled': 'float16',
        'Census_IsVirtualDevice': 'float16',
        'Census_IsTouchEnabled': 'int8',
        'Census_IsPenCapable': 'int8',
        'Census_IsAlwaysOnAlwaysConnectedCapable': 'float16',
        'Wdft_IsGamer': 'float16',
        'Wdft_RegionIdentifier': 'float16',
        'HasDetections': 'int8'
    }

    train = pd.read_csv('input/train.csv', dtype=dtypes, low_memory=True, nrows=num_row)
    train['MachineIdentifier'] = train.index.astype('uint32')
    test = pd.read_csv('input/test.csv', dtype=dtypes, low_memory=True, nrows=num_row)
    test['MachineIdentifier'] = test.index.astype('uint32')

    gc.collect()

    return train, test


def write_to_pickle(train, test):
    train.to_pickle('input/train.pkl')
    test.to_pickle('input/test.pkl')
    return None


def read_pickle():
    train = pd.read_pickle('input/train.pkl')
    test = pd.read_pickle('input/test.pkl')
    return train, test


def add_timestamp(train, test):
    datedictAS = np.load('input/AvSigVersionTimestamps.npy')[()]
    train['DateAS'] = train['AvSigVersion'].map(datedictAS)
    test['DateAS'] = test['AvSigVersion'].map(datedictAS)

    datedictOS = np.load('input/OSVersionTimestamps.npy')[()]
    train['DateOS'] = train['Census_OSVersion'].map(datedictOS)
    test['DateOS'] = test['Census_OSVersion'].map(datedictOS)

    def convert(x):
        try:
            d = datetime.strptime(x.split('.')[4], '%y%m%d-%H%M')
        except:
            d = np.nan
        return d

    train['DateBL'] = train['OsBuildLab'].map(convert)
    test['DateBL'] = test['OsBuildLab'].map(convert)

    del datedictAS, datedictOS
    gc.collect()

    return train, test


def split_column(train, test, col_name):
    temp = train[col_name].str.split('.', expand=True)
    temp.columns = [col_name + str(name) for name in temp.columns]
    train = pd.concat([train, temp], axis=1)

    temp = test[col_name].str.split('.', expand=True)
    temp.columns = [col_name + str(name) for name in temp.columns]
    test = pd.concat([test, temp], axis=1)

    del temp, train[col_name], test[col_name]
    gc.collect()

    return train, test


def initial_engineering(train, test):

    columns = ['EngineVersion',
               'AppVersion',
               'AvSigVersion',
               'Census_OSVersion',
               'OsBuildLab']
    for column in columns:
        train, test = split_column(train, test, column)

    gc.collect()

    return train, test


def drop_useless(train, test):
    droppable_features = [
         'Census_ProcessorClass',
         'Census_IsWIMBootEnabled',
         'IsBeta',
         'Census_IsFlightsDisabled',
         'Census_IsFlightingInternal',
         'AutoSampleOptIn',
         'Census_ThresholdOptIn',
         'SMode',
         'Census_IsPortableOperatingSystem',
         'PuaMode',
         'Census_DeviceFamily',
         'UacLuaenable',
         'Census_IsVirtualDevice',
         'Platform',
         'Census_OSSkuName',
         'Census_OSInstallLanguageIdentifier',
         'Processor'
    ]
    train.drop(droppable_features, inplace=True, axis=1)
    test.drop(droppable_features, inplace=True, axis=1)

    gc.collect()

    return train, test


def transfer_sparse(train, test):

    for usecol in train.columns.tolist():

        if usecol in ['MachineIdentifier', 'HasDetections']:
            continue

        train[usecol] = train[usecol].astype('str')
        test[usecol] = test[usecol].astype('str')

        # Fit LabelEncoder
        le = LabelEncoder().fit(
            np.unique(train[usecol].unique().tolist() +
                      test[usecol].unique().tolist()))

        # At the end 0 will be used for dropped values
        train[usecol] = le.transform(train[usecol]) + 1
        test[usecol] = le.transform(test[usecol]) + 1

        agg_tr = (train
                  .groupby([usecol])
                  .aggregate({'MachineIdentifier': 'count'})
                  .reset_index()
                  .rename(columns={'MachineIdentifier': 'Train'}))
        agg_te = (test
                  .groupby([usecol])
                  .aggregate({'MachineIdentifier': 'count'})
                  .reset_index()
                  .rename(columns={'MachineIdentifier': 'Test'}))

        agg = pd.merge(agg_tr, agg_te, on=usecol, how='outer').replace(np.nan, 0)
        # Select values with more than 1000 observations
        agg = agg[(agg['Train'] > 1000)].reset_index(drop=True)
        agg['Total'] = agg['Train'] + agg['Test']
        # Drop unbalanced values
        agg = agg[(agg['Train'] / agg['Total'] > 0.2) & (agg['Train'] / agg['Total'] < 0.8)]
        agg[usecol + 'Copy'] = agg[usecol]

        train[usecol] = (pd.merge(train[[usecol]],
                                  agg[[usecol, usecol + 'Copy']],
                                  on=usecol, how='left')[usecol + 'Copy']
                         .replace(np.nan, 0).astype('int').astype('category'))

        test[usecol] = (pd.merge(test[[usecol]],
                                 agg[[usecol, usecol + 'Copy']],
                                 on=usecol, how='left')[usecol + 'Copy']
                        .replace(np.nan, 0).astype('int').astype('category'))

        del le, agg_tr, agg_te, agg, usecol
        gc.collect()

    # y_train = np.array(train['HasDetections'])
    y_train = train['HasDetections']
    train_ids = train.index
    test_ids = test.index

    del train['HasDetections'], train['MachineIdentifier'], test['MachineIdentifier']
    gc.collect()

    return y_train, train_ids, test_ids, train, test


def add_noise(series, noise_level):
    return series * (1 + noise_level * np.random.randn(len(series)))


def target_encode(trn_series=None,
                  tst_series=None,
                  target=None,
                  min_samples_leaf=1,
                  smoothing=1,
                  noise_level=0):
    """
    Smoothing is computed like in the following paper by Daniele Micci-Barreca
    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf
    trn_series : training categorical feature as a pd.Series
    tst_series : test categorical feature as a pd.Series
    target : target data as a pd.Series
    min_samples_leaf (int) : minimum samples to take category average into account
    smoothing (int) : smoothing effect to balance categorical average vs prior
    """
    assert len(trn_series) == len(target)
    assert trn_series.name == tst_series.name
    temp = pd.concat([trn_series, target], axis=1)
    # Compute target mean
    averages = temp.groupby(by=trn_series.name)[target.name].agg(["mean", "count"])
    # Compute smoothing
    smoothing = 1 / (1 + np.exp(-(averages["count"] - min_samples_leaf) / smoothing))
    # Apply average function to all target data
    prior = target.mean()
    # The bigger the count the less full_avg is taken into account
    averages[target.name] = prior * (1 - smoothing) + averages["mean"] * smoothing
    averages.drop(["mean", "count"], axis=1, inplace=True)
    # Apply averages to trn and tst series
    ft_trn_series = pd.merge(
        trn_series.to_frame(trn_series.name),
        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),
        on=trn_series.name,
        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)
    # pd.merge does not keep the index so restore it
    ft_trn_series.index = trn_series.index
    ft_tst_series = pd.merge(
        tst_series.to_frame(tst_series.name),
        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),
        on=tst_series.name,
        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)
    # pd.merge does not keep the index so restore it
    ft_tst_series.index = tst_series.index
    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)


def one_hot(train, test):
    ohe = OneHotEncoder(sparse=True, dtype='uint8', categories='auto').fit(train)

    # Transform data using small groups to reduce memory usage
    m = 10 ** 5
    train = vstack([ohe.transform(train[i * m:(i + 1) * m]) for i in range(train.shape[0] // m + 1)])
    test = vstack([ohe.transform(test[i * m:(i + 1) * m]) for i in range(test.shape[0] // m + 1)])

    del ohe
    gc.collect()

    return train, test, m

from numba import jit

@jit
def fast_auc(y_true, y_prob):
    y_true = np.asarray(y_true)
    y_true = y_true[np.argsort(y_prob)]
    nfalse = 0
    auc = 0
    n = len(y_true)
    for i in range(n):
        y_i = y_true[i]
        nfalse += (1 - y_i)
        auc += y_i * nfalse
    auc /= (nfalse * (n - nfalse))
    return auc


def eval_auc(preds, dtrain):
    labels = dtrain.get_label()
    return 'auc', fast_auc(labels, preds), True


def k_fold_lgbm(train, test, y_train, train_ids, test_ids, m, debug, type='gbdt'):

    counter = 0
    y_train = np.array(y_train)
    num_fold = 2 if debug else 5
    skf = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=777)
    skf.get_n_splits(train_ids, y_train)

    lgb_test_result = np.zeros(test_ids.shape[0])
    lgb_train_result = np.zeros(train_ids.shape[0])

    print('\nLightGBM\n')

    for train_index, test_index in skf.split(train_ids, y_train):

        print('\nFold {}\n'.format(counter + 1))

        X_fit = vstack([train[train_index[i * m:(i + 1) * m]] for i in range(train_index.shape[0] // m + 1)])
        X_val = vstack([train[test_index[i * m:(i + 1) * m]] for i in range(test_index.shape[0] // m + 1)])
        X_fit, X_val = csr_matrix(X_fit, dtype='float32'), csr_matrix(X_val, dtype='float32')
        y_fit, y_val = y_train[train_index], y_train[test_index]
        scores = []

        # {'bagging_fraction': 0.9217215430264205, 'feature_fraction': 0.5832133948246924,
        #  'learning_rate': 0.049588047174152064, 'max_depth': 14.177215831379076, 'num_leaves': 92.56913079322054}

        # TODO use learning of 0.02 if time permits
        # lgb_model = lgb.LGBMClassifier(boosting_type=type,
        #                                max_depth=14,
        #                                n_estimators=30000,
        #                                learning_rate=0.05,
        #                                num_leaves=93,
        #                                colsample_bytree=0.28,
        #                                objective='binary',
        #                                n_jobs=-1,
        #                                reg_alpha=1,
        #                                feature_fraction=0.58,
        #                                bagging_fraction=0.92,
        #                                # reg_lambda=0.1,
        #                                subsample_freq=1
        #                                )

        lgb_model = lgb.LGBMClassifier(boosting_type=type,
                                       max_depth=-1,
                                       n_estimators=30000,
                                       learning_rate=0.05,
                                       num_leaves=2 ** 12 - 1,
                                       colsample_bytree=0.28,
                                       objective='binary',
                                       n_jobs=-1,
                                       reg_alpha=1,
                                       # reg_lambda=0.1,
                                       subsample_freq=1,
                                       # device='gpu',
                                       # gpu_platform_id=0,
                                       # gpu_device_id=0,
                                       # gpu_use_dp=False
                                       )

        lgb_model.fit(X_fit, y_fit,
                      eval_metric='auc',
                      # eval_metric=eval_auc, not working for fit
                      eval_set=[(X_val, y_val)],
                      verbose=100, early_stopping_rounds=100)


        lgb_train_result[test_index] += lgb_model.predict_proba(X_val)[:,1]

        scores.append(lgb_model.best_score_['valid_0']['auc'])

        del X_fit, X_val, y_fit, y_val, train_index, test_index
        gc.collect()

        test = csr_matrix(test, dtype='float32')
        lgb_test_result += lgb_model.predict_proba(test)[:, 1]
        counter += 1

    score = np.mean(scores)
    lgb_train_result = lgb_train_result/num_fold
    lgb_test_result = lgb_test_result/num_fold

    return lgb_train_result, lgb_test_result, round(score, 3)


def k_fold_rf(train, test, y_train, train_ids, test_ids, m, debug):
    counter = 0
    y_train = np.array(y_train)
    num_fold = 2 if debug else 5
    skf = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=777)
    skf.get_n_splits(train_ids, y_train)

    rf_test_result = np.zeros(test_ids.shape[0])
    rf_train_result = np.zeros(train_ids.shape[0])

    print('\nRandomForest\n')

    for train_index, test_index in skf.split(train_ids, y_train):
        print('\nFold {}\n'.format(counter + 1))

        X_fit = vstack([train[train_index[i * m:(i + 1) * m]] for i in range(train_index.shape[0] // m + 1)])
        X_val = vstack([train[test_index[i * m:(i + 1) * m]] for i in range(test_index.shape[0] // m + 1)])
        X_fit, X_val = csr_matrix(X_fit, dtype='float32'), csr_matrix(X_val, dtype='float32')
        y_fit, y_val = y_train[train_index], y_train[test_index]
        scores = []

        rand_forest = RandomForestClassifier(n_estimators=1000, min_samples_leaf=20, n_jobs=-1)

        rand_forest.fit(X_fit, y_fit)

        rf_train_result[test_index] += rand_forest.predict_proba(X_val)[:, 1]

        scores.append(fast_auc(y_val, rf_train_result[test_index]))

        del X_fit, X_val, y_fit, y_val, train_index, test_index
        gc.collect()

        test = csr_matrix(test, dtype='float32')
        rf_test_result += rand_forest.predict_proba(test)[:, 1]
        counter += 1

    score = np.mean(scores)
    lgb_train_result = rf_train_result / num_fold
    lgb_test_result = rf_test_result / num_fold

    return lgb_train_result, lgb_test_result, round(score, 3)


def k_fold_ada(train, test, y_train, train_ids, test_ids, m, debug):
    counter = 0
    y_train = np.array(y_train)
    num_fold = 2 if debug else 5
    skf = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=777)
    skf.get_n_splits(train_ids, y_train)

    ada_test_result = np.zeros(test_ids.shape[0])
    ada_train_result = np.zeros(train_ids.shape[0])

    print('\nAdaBoost\n')

    for train_index, test_index in skf.split(train_ids, y_train):
        print('\nFold {}\n'.format(counter + 1))

        X_fit = vstack([train[train_index[i * m:(i + 1) * m]] for i in range(train_index.shape[0] // m + 1)])
        X_val = vstack([train[test_index[i * m:(i + 1) * m]] for i in range(test_index.shape[0] // m + 1)])
        X_fit, X_val = csr_matrix(X_fit, dtype='float32'), csr_matrix(X_val, dtype='float32')
        y_fit, y_val = y_train[train_index], y_train[test_index]
        scores = []

        rand_forest = RandomForestClassifier(n_estimators=500, min_samples_leaf=20, n_jobs=-1)

        ada_boost = AdaBoostClassifier(
            base_estimator=rand_forest,
            n_estimators=6
        )


        ada_boost.fit(X_fit, y_fit, )

        ada_train_result[test_index] += ada_boost.predict_proba(X_val)[:, 1]

        scores.append(fast_auc(y_val, ada_train_result[test_index]))

        del X_fit, X_val, y_fit, y_val, train_index, test_index
        gc.collect()

        test = csr_matrix(test, dtype='float32')
        ada_test_result += ada_boost.predict_proba(test)[:, 1]
        counter += 1

    score = np.mean(scores)
    lgb_train_result = ada_train_result / num_fold
    lgb_test_result = ada_test_result / num_fold

    return lgb_train_result, lgb_test_result, round(score, 3)


def k_fold_xgb(train, test, y_train, train_ids, test_ids, m, debug):

    y_train = np.array(y_train)
    num_fold = 2 if debug else 5
    skf = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=777)
    skf.get_n_splits(train_ids, y_train)

    xgb_test_result = np.zeros(test_ids.shape[0])
    xgb_train_result = np.zeros(train_ids.shape[0])
    counter = 0

    print('\nXgb\n')

    for train_index, test_index in skf.split(train_ids, y_train):
        print('Fold {}\n'.format(counter + 1))

        X_fit = vstack([train[train_index[i * m:(i + 1) * m]] for i in range(train_index.shape[0] // m + 1)])
        X_val = vstack([train[test_index[i * m:(i + 1) * m]] for i in range(test_index.shape[0] // m + 1)])
        X_fit, X_val = csr_matrix(X_fit, dtype='float32'), csr_matrix(X_val, dtype='float32')
        y_fit, y_val = y_train[train_index], y_train[test_index]
        scores = []

        xgb_model = xgb.XGBClassifier(max_depth=6,
                                      n_estimators=3000,
                                      colsample_bytree=0.2,
                                      learning_rate=0.05,
                                      objective='binary:logistic',
                                      n_jobs=-1,
                                      # tree_method='gpu_hist'
                                      )

        xgb_model.fit(X_fit, y_fit, eval_metric='auc',
                      eval_set=[(X_val, y_val)],
                      verbose=100,
                      early_stopping_rounds=300)

        xgb_train_result[test_index] += xgb_model.predict_proba(X_val)[:,1]

        scores.append(xgb_model.best_score)

        del X_fit, X_val, y_fit, y_val, train_index, test_index
        gc.collect()

        test = csr_matrix(test, dtype='float32')

        xgb_test_result += xgb_model.predict_proba(test)[:, 1]
        counter += 1

    score = np.mean(scores)
    xgb_test_result = xgb_test_result/num_fold
    xgb_train_result = xgb_test_result/num_fold

    return xgb_train_result, xgb_test_result, round(score, 3)


def save_submission(lgb_test_result, score, model_name):
    submission = pd.read_csv('input/sample_submission.csv')
    submission['HasDetections'] = lgb_test_result
    file_name = 'output/' + model_name + '_submission_' + str(score) + '.csv'
    submission.to_csv(file_name, index=False)
    return None


def save_to_pickle(data, PIK):
    with open(PIK, "wb") as f:
        pickle.dump(data, f, protocol=4)
    return


def read_from_pickle(PIK):
    gc.disable()
    with open(PIK, "rb") as f:
        data = pickle.load(f)
    gc.enable()

    return data
