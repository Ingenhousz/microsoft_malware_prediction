from util import *
from feature_engineering import engineer_feature
from bayes_opt import BayesianOptimization
from bayes_opt.observer import JSONLogger
from bayes_opt.event import Events

debug = False
seed = 777
folds = 5
n_rows = 1000000 if not debug else 100

train, test, y_train, train_ids, test_ids, m = engineer_feature(debug)

ind = np.random.choice(test.shape[0], n_rows, replace=False)
train = train[ind, :]
y_train = y_train[ind]

# train = train.sample(n_rows, random_state=777)
# test = test.sample(n_rows, random_state=777)
train = train.astype('float32')
y_train = y_train.astype('float32')

del test, train_ids, test_ids
gc.collect()

lgbm_dataset = lgb.Dataset(data=train, label=y_train)

# Specify LightGBM Cross Validation function
def lgbm_cv_evaluator(learning_rate, num_leaves, feature_fraction, bagging_fraction, max_depth):
    # Setup Parameters
    params = {'objective': 'binary',
              'boosting': 'gbdt',
              'num_iterations': 1250,
              'early_stopping_round': 100,
              'metric': 'auc',
              'verbose': -1
              }
    params['learning_rate'] = learning_rate
    params['num_leaves'] = int(round(num_leaves))
    params['feature_fraction'] = max(min(feature_fraction, 1), 0)
    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)
    params['max_depth'] = int(round(max_depth))

    # Run LightGBM Cross Validation
    result = lgb.cv(params, lgbm_dataset, nfold=folds, seed=seed,
                     stratified=True, verbose_eval=-1, metrics=['auc'])

    # Return AUC
    return max(result['auc-mean'])


def display_progress(event, instance):
    iter = len(instance.res) - 1
    print('Iteration: {} - AUC: {} - {}'.format(iter, instance.res[iter].get('target'), instance.res[iter].get('params')))


def bayesian_parameter_optimization(init_rounds=1, opt_rounds=1):
    # Initialize Bayesian Optimization
    optimizer = BayesianOptimization(f=lgbm_cv_evaluator,
                                     pbounds={'learning_rate': (0.02, 0.06),
                                              'num_leaves': (20, 100),
                                              'feature_fraction': (0.25, 0.75),
                                              'bagging_fraction': (0.75, 0.95),
                                              'max_depth': (8, 15)},
                                     random_state=seed,
                                     verbose=2)

    # Subscribe Logging to file for each Optimization Step
    logger = JSONLogger(path='parameter_output.json')
    optimizer.subscribe(Events.OPTMIZATION_STEP, logger)

    # Subscribe the custom display_progress function for each Optimization Step
    optimizer.subscribe(Events.OPTMIZATION_STEP, " ", display_progress)

    # Perform Bayesian Optimization.
    # Modify acq, kappa and xi to change the behaviour of Bayesian Optimization itself.
    optimizer.maximize(init_points=init_rounds, n_iter=opt_rounds, acq="ei", kappa=2, xi=0.1)

    # Return Found Best Parameter values and Target
    return optimizer.max


max_params = bayesian_parameter_optimization(init_rounds = 15, opt_rounds = 15)

print('================= Results')
print('Found Max AUC: {} with the following Parameters: '.format(max_params.get('target')))
print(max_params.get('params'))